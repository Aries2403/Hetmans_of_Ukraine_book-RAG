import streamlit as st
import json
import os
import chromadb
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from config import settings

# === –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è ===
CACHE_FILE = "cache.json"
CHROMA_PATH = settings.CHROMA_PATH
COLLECTION_NAME = settings.COLLECTION_NAME
EMBEDDING_MODEL = settings.EMBEDDING_MODEL
TOP_K = settings.TOP_K

if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, 'r', encoding='utf-8') as f:
        cache = json.load(f)
else:
    cache = {}

# === CSS –î–ò–ó–ê–ô–ù (–ë–ï–ó–ü–ï–ß–ù–ê –í–ï–†–°–Ü–Ø) ===
st.markdown("""
<style>
    .stApp {
        background: linear-gradient(180deg, #0057b7 0%, #004c99 30%, #ffd700 100%) !important;
    }

    h1 {
        color: #ffd700 !important;
        text-align: center;
        font-size: 3rem !important;
        text-shadow: 3px 3px 6px rgba(0,0,0,0.7);
        margin-bottom: 0.5rem !important;
    }

    .subtitle {
        color: #ffffff;
        text-align: center;
        font-size: 1.1rem;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
    }
</style>
""", unsafe_allow_html=True)

# === –û–ë–ö–õ–ê–î–ò–ù–ö–ê + –ó–ê–ì–û–õ–û–í–û–ö –í –û–î–ù–û–ú–£ –ö–û–ù–¢–ï–ô–ù–ï–†–Ü ===
col1, col2 = st.columns([1, 5])
with col1:
    if os.path.exists("book_cover.jpg"):
        st.image("book_cover.jpg", width=100)
with col2:
    st.markdown("<h1>üá∫üá¶ –£—Å—ñ –ì–µ—Ç—å–º–∞–Ω–∏ –£–∫—Ä–∞—ó–Ω–∏</h1>", unsafe_allow_html=True)
    st.markdown("""
    <p class="subtitle">RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É –ø–æ –∫–Ω–∏–∑—ñ "–£—Å—ñ –≥–µ—Ç—å–º–∞–Ω–∏ –£–∫—Ä–∞—ó–Ω–∏"<br>–∞–≤—Ç. –†–µ–µ–Ω—Ç –û.–ü., –ö–æ–ª—è–¥–∞ –Ü.–ê. ¬∑ 2008</p>
    """, unsafe_allow_html=True)


# === –£–ù–Ü–ö–ê–õ–¨–ù–Ü–°–¢–¨ ===
def deduplicate_by_id(results):
    seen = set()
    unique = []
    for meta, dist, doc in zip(results["metadatas"][0], results["distances"][0], results["documents"][0]):
        chunk_id = f"{meta['doc_path']}#{meta['chunk_number']}"
        if chunk_id not in seen:
            seen.add(chunk_id)
            unique.append((meta, dist, doc))
    return unique


# === LLM ===
def generate_response(query, context_chunks):
    context = "\n".join([f"[{i + 1}] {chunk}" for i, (_, _, chunk) in enumerate(context_chunks)])

    prompt = f"""–¢–∏ ‚Äî –µ–∫—Å–ø–µ—Ä—Ç –∑ —ñ—Å—Ç–æ—Ä—ñ—ó –£–∫—Ä–∞—ó–Ω–∏. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò –Ω–∞–≤–µ–¥–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ (1-3 —Ä–µ—á–µ–Ω–Ω—è), –∑ –Ω–æ–º–µ—Ä–∞–º–∏ –¥–∂–µ—Ä–µ–ª [1], [2] —Ç–æ—â–æ.
–Ø–∫—â–æ –Ω–µ –≤–ø–µ–≤–Ω–µ–Ω–∏–π ‚Äî —Å–∫–∞–∂–∏: "–ù–µ–º–∞—î —Ç–æ—á–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ".

–ó–∞–ø–∏—Ç: {query}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í—ñ–¥–ø–æ–≤—ñ–¥—å:"""

    try:
        client = OpenAI(api_key=settings.OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=200
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"–ü–æ–º–∏–ª–∫–∞ LLM: {e}"


# === RAG ===
def rag_query(query):
    model = SentenceTransformer(EMBEDDING_MODEL)
    client = chromadb.PersistentClient(path=CHROMA_PATH)

    try:
        collection = client.get_collection(COLLECTION_NAME)
    except:
        return "‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —â–µ –Ω–µ –≥–æ—Ç–æ–≤–∞. –°–ø—Ä–æ–±—É–π—Ç–µ –ø—ñ–∑–Ω—ñ—à–µ.", []

    query_emb = model.encode([query]).tolist()
    results = collection.query(
        query_embeddings=query_emb,
        n_results=TOP_K,
        include=["documents", "metadatas", "distances"]
    )

    unique_results = deduplicate_by_id(results)
    top_chunks = unique_results[:3]

    response = generate_response(query, top_chunks)

    sources = []
    for i, (meta, _, _) in enumerate(top_chunks, 1):
        sources.append(f"[{i}] {meta['doc_name']} (—á–∞–Ω–∫ {meta['chunk_number']})")

    return response, sources


# === –Ü–°–¢–û–†–Ü–Ø ===
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "sources" in msg:
            with st.expander("–î–∂–µ—Ä–µ–ª–∞"):
                for s in msg["sources"]:
                    st.write(s)
        if "image" in msg and msg["image"]:
            st.image(msg["image"], width=200)

# === –í–í–Ü–î ===
if query := st.chat_input("–ó–∞–ø–∏—Ç–∞–π –ø—Ä–æ –≥–µ—Ç—å–º–∞–Ω–∞ –∞–±–æ –Ω–∞–ø–∏—à–∏ —Ñ–æ—Ç–æ –Ü–º'—è –ü—Ä—ñ–∑–≤–∏—â–µ"):
    query = query.strip()
    if not query:
        st.warning("–í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç–∞–Ω–Ω—è.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        with st.spinner("–®—É–∫–∞—é..."):
            answer = ""
            sources = []
            image_path = None

            # === –ö–û–ú–ê–ù–î–ê: —Ñ–æ—Ç–æ –Ü–º'—è –ü—Ä—ñ–∑–≤–∏—â–µ ===
            if query.lower().startswith("—Ñ–æ—Ç–æ "):
                name = query[5:].strip()
                for ext in ['.jpg', '.jpeg', '.png']:
                    candidate = f"images/hetmans/{name}{ext}"
                    if os.path.exists(candidate):
                        image_path = candidate
                        break

                if image_path:
                    try:
                        st.image(image_path, caption=name, width=200)
                        answer = f"–û—Å—å —Ñ–æ—Ç–æ: **{name}**"
                    except Exception as e:
                        answer = f"‚ö†Ô∏è –§–æ—Ç–æ —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–µ. –ü–æ–º–∏–ª–∫–∞: {type(e).__name__}"
                else:
                    answer = "–§–æ—Ç–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ."

            # === RAG-–∑–∞–ø–∏—Ç ===
            else:
                if query in cache:
                    answer = cache[query]["answer"]
                    sources = cache[query]["sources"]
                else:
                    answer, sources = rag_query(query)
                    cache[query] = {"answer": answer, "sources": sources}
                    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                        json.dump(cache, f, ensure_ascii=False, indent=2)

                st.markdown(answer)
                with st.expander("–î–∂–µ—Ä–µ–ª–∞"):
                    for s in sources:
                        st.write(s)

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "sources": sources,
                "image": image_path
            })